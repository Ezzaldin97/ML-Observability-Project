# Batch Scoring ML Observability Project:

## Description:

Create Monitoring System/Reports for ML Model in Production to Track the Model Performance in Production, Detect the Data Drift in Production and How it can affect the Model Performance, and check Data Integrity/Quality of the Data.

## Objective:

Getting Familiar with ML Observaility Concept, Use Evidently AI Package in Python which provides alot Metrics and Tests to Create Automatic Reports about Model Performance and Data, Simulate How to Create Batch Scoring Jobs/Data (Drift, Quality, Integrity) Jobs/Feedback of Batch Scoring Model Performance in Production, and Alert the Results Using Email.

Optional We Can do Automatic retraining for the Model.

## Prerequisites:

- Git Bash in Windows or Linux OS
- Python >= 3.7 (I Used 3.7.9)
- Knowledge in ML

## Environment Setup:

## Use Case

## References:

- [DataTalks MLOps ZoomCamp Course](https://www.youtube.com/watch?v=3T5kUA3eWWc&list=PL3MmuxUbc_hIUISrluw_A7wDSmfOhErJK)
- [ML Observability Workshop](https://github.com/alexeygrigorev/ml-observability-workshop)
- [ML Observability Article](https://towardsdatascience.com/what-is-ml-observability-29e85e701688)
- [Evidently AI Data Drift Measures](https://www.evidentlyai.com/blog/data-drift-detection-large-datasets)
